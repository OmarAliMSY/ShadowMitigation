{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install opencv-python ipywidgets imageio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from Seq2Seq import Seq2Seq\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "#os.environ[\"TORCH_DEVICE\"] = \"cuda\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "p = Path(r'C:\\Users\\Omar\\Documents\\ShadowMitigation\\SKIPPD')\n",
    "#device = os.environ.get(\"TORCH_DEVICE\", 'cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = {}\n",
    "for path in Path(p, 'Cloudmask').rglob('*.jpg'):\n",
    "    x = path.stem\n",
    "\n",
    "    year, month, day, hour, minute = x[:4], x[4:6], x[6:8],x[8:10],x[10:12]\n",
    "    \n",
    "    key = f'{year}-{month}-{day}-{hour}'\n",
    "    if key not in dataset.keys():\n",
    "        dataset[key] = []\n",
    "    \n",
    "    dataset[key].append(path)\n",
    "    \n",
    "for key, value in dataset.items():\n",
    "    dataset[key] = sorted(value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty = []\n",
    "correct = []\n",
    "for k, v in dataset.items():\n",
    "    if len(v) != 60:\n",
    "        \n",
    "        faulty.append(k)\n",
    "    else:\n",
    "        correct.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1251, 60, 64, 64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "SKIPPD = []\n",
    "for valid in correct:\n",
    "    #print(valid)\n",
    "    SKIPPD.append([cv2.imread(str(i), cv2.IMREAD_GRAYSCALE) for i in dataset[valid]])\n",
    "print(np.array(SKIPPD).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SKIPPD is a numpy array with shape (1251, 60, 64, 64)\n",
    "threshold = 20000*255  # Example threshold value\n",
    "\n",
    "# Calculate the sum of pixel values for each sequence\n",
    "sequence_sums = np.sum(SKIPPD, axis=(1, 2, 3))\n",
    "\n",
    "# Filter sequences where the sum is above the threshold\n",
    "filtered_indices = np.where(sequence_sums > threshold)[0]\n",
    "filtered_SKIPPD = np.array(SKIPPD)[filtered_indices]\n",
    "\n",
    "print(f\"Original shape: {np.array(SKIPPD).shape}\")\n",
    "print(f\"Filtered shape: {filtered_SKIPPD.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "SKIPPD_SHORT = SKIPPD\n",
    "# Shuffle Data\n",
    "np.random.shuffle(SKIPPD_SHORT)\n",
    "\n",
    "\n",
    "# Train, Test, Validation splits\n",
    "train_data = SKIPPD_SHORT[:int(0.7*len(SKIPPD_SHORT))]       \n",
    "val_data = SKIPPD_SHORT[int(0.7*len(SKIPPD_SHORT)):int(0.9*len(SKIPPD_SHORT))]       \n",
    "test_data = SKIPPD_SHORT[int(0.9*len(SKIPPD_SHORT)):]     \n",
    "\n",
    "def collate(batch, device='cuda', threshold=1000, max_attempts=15):\n",
    "    # Convert batch to tensor, add channel dimension, and scale\n",
    "    batch = torch.tensor(batch).unsqueeze(1) / 255.0\n",
    "    batch = batch.to(device)\n",
    "\n",
    "    for _ in range(max_attempts):\n",
    "        # Randomly pick a sequence\n",
    "        rand = np.random.randint(15, 59)\n",
    "        input_seq = batch[:, :, rand-15:rand]\n",
    "        target = batch[:, :, rand+1]\n",
    "       \n",
    "        # Check if the sum of the sequence exceeds the threshold\n",
    "        if input_seq.sum() > threshold:\n",
    "\n",
    "            return input_seq, target\n",
    "\n",
    "    return input_seq, target\n",
    "\n",
    "# Example usage (adjust as per your actual 'device' and 'batch' data)\n",
    "\n",
    "\n",
    "# # Training Data Loader\n",
    "train_loader = DataLoader(train_data, shuffle=True, \n",
    "                        batch_size=16, collate_fn=collate)\n",
    "\n",
    "# Validation Data Loader\n",
    "val_loader = DataLoader(val_data, shuffle=True, \n",
    "                        batch_size=16, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "for hour in faulty:\n",
    "    images =[cv2.imread(str(i), cv2.IMREAD_GRAYSCALE) for i in dataset[hour]]\n",
    "\n",
    "    for image in images:\n",
    "        print(image.shape)\n",
    "    # fig, axs = plt.subplots(8, 8, figsize=(12, 12))\n",
    "\n",
    "    # for i, ax in enumerate(axs.flat):\n",
    "    #     if i < len(images):\n",
    "    #         ax.imshow(images[i], cmap='gray')\n",
    "    #     ax.axis('off')\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10a02931398d47ceb4efe8c781e9b688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x83\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e43d949c7fe941e28faf732b490c5523",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x83\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f7eda8a9709458ea4bdab53e35304e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c903557367d48ac8e7a386756c182db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x82\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ec2e7790dfd4477b1e6bf2e9737750f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db890651a2df43cca81201aa2ed04a20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x83\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b556f647ce8541509b80bb09d7a638dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "794310ff359d4bb6902ccdd8d5481d1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3152d149c26f41a88c98d0403ad07fbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f31be5afea44abcb8b7576886aeead3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'GIF89a@\\x00@\\x00\\x84\\x00\\x00\\x00\\x00\\x00\\x01\\x01\\x01\\x02\\x02\\x02\\x03\\x03\\x03\\x04\\…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import io\n",
    "import imageio\n",
    "from ipywidgets import widgets, HBox\n",
    "\n",
    "# Get a batch\n",
    "input, _ = next(iter(val_loader))\n",
    "\n",
    "# Reverse process before displaying\n",
    "input = input.cpu().numpy() * 255.0     \n",
    "\n",
    "for video in input.squeeze(1)[:10]:          # Loop over videos\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif,video.astype(np.uint8),\"GIF\",fps=5)\n",
    "        display(HBox([widgets.Image(value=gif.getvalue())]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get a batch\n",
    "for input_seq, target in val_loader:\n",
    "    # Reverse process before displaying\n",
    "    input_seq = input_seq.cpu().numpy() * 255.0\n",
    "    target = target.cpu().numpy() * 255.0\n",
    "    \n",
    "    print(\"Input sequence shape:\", input_seq.shape)\n",
    "    print(\"Target shape:\", target.shape)\n",
    "    \n",
    "     #Adjust the figure size as needed\n",
    "    plt.figure(figsize=(20, 8))\n",
    "\n",
    "    # Displaying the input sequence frames\n",
    "    for i in range(input_seq.shape[2]):  # input_seq.shape[2] should be 15 for the sequence length\n",
    "        plt.subplot(4, 4, i+1)  # Adjust the grid to 4x4 to fit all 16 images\n",
    "        plt.imshow(input_seq[0, 0, i], cmap='gray')\n",
    "        plt.title(f'Input Frame {i+1}')\n",
    "        plt.axis('off')\n",
    "\n",
    "    # Displaying the target frame\n",
    "    plt.subplot(4, 4, 16)  # Position for the target frame in the 4x4 grid\n",
    "    plt.imshow(target[0,0], cmap='gray')\n",
    "    plt.title('Target Frame')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    break  # Only process the first batch for demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "device = 'cuda'\n",
    "# The input video frames are grayscale, thus single channel\n",
    "model = Seq2Seq(num_channels=1, num_kernels=64, \n",
    "kernel_size=(3, 3), padding=(1, 1), activation=\"relu\", \n",
    "frame_size=(64, 64), num_layers=3).to(device)\n",
    "lr=1e-4\n",
    "optim = Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Binary Cross Entropy, target pixel values either 0 or 1\n",
    "criterion = nn.BCELoss(reduction='sum')\n",
    "min_loss = float('inf')\n",
    "scheduler = ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    # set the wandb project where this run will be logged\n",
    "    project=\"ShadowMitigation\",\n",
    "    \n",
    "    # track hyperparameters and run metadata\n",
    "    config={\n",
    "    \"learning_rate\": 1e-7,\n",
    "    \"architecture\": \"CONVLSTM - 3 Layers\",\n",
    "    \"dataset\": \"0.3 SKIPPD\",\n",
    "    \"epochs\": 100,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = torch.load(r'C:\\Users\\Omar\\Documents\\ShadowMitigation\\playground\\pyt\\best.pth')\n",
    "model.load_state_dict(asd['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "    start = time.time()\n",
    "    train_loss = 0                                                 \n",
    "    model.train()                                                  \n",
    "    for batch_num, (input_seq, target) in enumerate(tqdm(train_loader), 1):  \n",
    "        batch_start = time.time()\n",
    "        output = model(input_seq)          \n",
    "                                   \n",
    "        loss = criterion(output.flatten(), target.flatten())       \n",
    "                           \n",
    "        loss.backward()                                            \n",
    "        optim.step()                                               \n",
    "        optim.zero_grad()  \n",
    "                       \n",
    "\n",
    "        # train_loss += loss.item()\n",
    "\n",
    "\n",
    "    train_loss /= len(train_loader.dataset)                       \n",
    "    val_loss = 0                                                 \n",
    "    model.eval()                                                   \n",
    "    with torch.no_grad():                                          \n",
    "        for input, target in val_loader:                          \n",
    "            output = model(input)                                   \n",
    "            loss = criterion(output.flatten(), target.flatten())   \n",
    "            val_loss += loss.item()                                \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    if val_loss < min_loss:\n",
    "        if val_loss < min_loss or min_loss == 0:                        \n",
    "            ts = str(time.time()).split(\".\")[0]\n",
    "            checkpoint = {\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optim.state_dict(),\n",
    "                }\n",
    "            torch.save(checkpoint, f'best_240220_2.pth')\n",
    "            min_loss = val_loss\n",
    "            \n",
    "    wandb.log({\"loss\": val_loss,\"val_loss_perc\":val_loss/(64*64) *100})\n",
    "    print(\"Epoch:{} Training Loss:{:.2f} Validation Loss:{:.2f}\\n  Validation Loss percentage: {:.2f}    Time: {:.2f}\".format(\n",
    "       epoch, train_loss, val_loss, val_loss/(64*64) *100,time.time()-start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['model.enc.enc.0.conv.conv.weight', 'model.enc.enc.0.conv.conv.bias', 'model.enc.enc.0.conv.norm.weight', 'model.enc.enc.0.conv.norm.bias', 'model.enc.enc.1.conv.conv.weight', 'model.enc.enc.1.conv.conv.bias', 'model.enc.enc.1.conv.norm.weight', 'model.enc.enc.1.conv.norm.bias', 'model.enc.enc.2.conv.conv.weight', 'model.enc.enc.2.conv.conv.bias', 'model.enc.enc.2.conv.norm.weight', 'model.enc.enc.2.conv.norm.bias', 'model.enc.enc.3.conv.conv.weight', 'model.enc.enc.3.conv.conv.bias', 'model.enc.enc.3.conv.norm.weight', 'model.enc.enc.3.conv.norm.bias', 'model.dec.dec.0.conv.conv.0.weight', 'model.dec.dec.0.conv.conv.0.bias', 'model.dec.dec.0.conv.norm.weight', 'model.dec.dec.0.conv.norm.bias', 'model.dec.dec.1.conv.conv.weight', 'model.dec.dec.1.conv.conv.bias', 'model.dec.dec.1.conv.norm.weight', 'model.dec.dec.1.conv.norm.bias', 'model.dec.dec.2.conv.conv.0.weight', 'model.dec.dec.2.conv.conv.0.bias', 'model.dec.dec.2.conv.norm.weight', 'model.dec.dec.2.conv.norm.bias', 'model.dec.dec.3.conv.conv.weight', 'model.dec.dec.3.conv.conv.bias', 'model.dec.dec.3.conv.norm.weight', 'model.dec.dec.3.conv.norm.bias', 'model.dec.readout.weight', 'model.dec.readout.bias', 'model.hid.enc.0.block.layer_scale_1', 'model.hid.enc.0.block.layer_scale_2', 'model.hid.enc.0.block.norm1.weight', 'model.hid.enc.0.block.norm1.bias', 'model.hid.enc.0.block.norm1.running_mean', 'model.hid.enc.0.block.norm1.running_var', 'model.hid.enc.0.block.norm1.num_batches_tracked', 'model.hid.enc.0.block.attn.proj_1.weight', 'model.hid.enc.0.block.attn.proj_1.bias', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.0.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.0.block.attn.proj_2.weight', 'model.hid.enc.0.block.attn.proj_2.bias', 'model.hid.enc.0.block.norm2.weight', 'model.hid.enc.0.block.norm2.bias', 'model.hid.enc.0.block.norm2.running_mean', 'model.hid.enc.0.block.norm2.running_var', 'model.hid.enc.0.block.norm2.num_batches_tracked', 'model.hid.enc.0.block.mlp.fc1.weight', 'model.hid.enc.0.block.mlp.fc1.bias', 'model.hid.enc.0.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.0.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.0.block.mlp.fc2.weight', 'model.hid.enc.0.block.mlp.fc2.bias', 'model.hid.enc.0.reduction.weight', 'model.hid.enc.0.reduction.bias', 'model.hid.enc.1.block.layer_scale_1', 'model.hid.enc.1.block.layer_scale_2', 'model.hid.enc.1.block.norm1.weight', 'model.hid.enc.1.block.norm1.bias', 'model.hid.enc.1.block.norm1.running_mean', 'model.hid.enc.1.block.norm1.running_var', 'model.hid.enc.1.block.norm1.num_batches_tracked', 'model.hid.enc.1.block.attn.proj_1.weight', 'model.hid.enc.1.block.attn.proj_1.bias', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.1.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.1.block.attn.proj_2.weight', 'model.hid.enc.1.block.attn.proj_2.bias', 'model.hid.enc.1.block.norm2.weight', 'model.hid.enc.1.block.norm2.bias', 'model.hid.enc.1.block.norm2.running_mean', 'model.hid.enc.1.block.norm2.running_var', 'model.hid.enc.1.block.norm2.num_batches_tracked', 'model.hid.enc.1.block.mlp.fc1.weight', 'model.hid.enc.1.block.mlp.fc1.bias', 'model.hid.enc.1.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.1.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.1.block.mlp.fc2.weight', 'model.hid.enc.1.block.mlp.fc2.bias', 'model.hid.enc.2.block.layer_scale_1', 'model.hid.enc.2.block.layer_scale_2', 'model.hid.enc.2.block.norm1.weight', 'model.hid.enc.2.block.norm1.bias', 'model.hid.enc.2.block.norm1.running_mean', 'model.hid.enc.2.block.norm1.running_var', 'model.hid.enc.2.block.norm1.num_batches_tracked', 'model.hid.enc.2.block.attn.proj_1.weight', 'model.hid.enc.2.block.attn.proj_1.bias', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.2.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.2.block.attn.proj_2.weight', 'model.hid.enc.2.block.attn.proj_2.bias', 'model.hid.enc.2.block.norm2.weight', 'model.hid.enc.2.block.norm2.bias', 'model.hid.enc.2.block.norm2.running_mean', 'model.hid.enc.2.block.norm2.running_var', 'model.hid.enc.2.block.norm2.num_batches_tracked', 'model.hid.enc.2.block.mlp.fc1.weight', 'model.hid.enc.2.block.mlp.fc1.bias', 'model.hid.enc.2.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.2.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.2.block.mlp.fc2.weight', 'model.hid.enc.2.block.mlp.fc2.bias', 'model.hid.enc.3.block.layer_scale_1', 'model.hid.enc.3.block.layer_scale_2', 'model.hid.enc.3.block.norm1.weight', 'model.hid.enc.3.block.norm1.bias', 'model.hid.enc.3.block.norm1.running_mean', 'model.hid.enc.3.block.norm1.running_var', 'model.hid.enc.3.block.norm1.num_batches_tracked', 'model.hid.enc.3.block.attn.proj_1.weight', 'model.hid.enc.3.block.attn.proj_1.bias', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.3.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.3.block.attn.proj_2.weight', 'model.hid.enc.3.block.attn.proj_2.bias', 'model.hid.enc.3.block.norm2.weight', 'model.hid.enc.3.block.norm2.bias', 'model.hid.enc.3.block.norm2.running_mean', 'model.hid.enc.3.block.norm2.running_var', 'model.hid.enc.3.block.norm2.num_batches_tracked', 'model.hid.enc.3.block.mlp.fc1.weight', 'model.hid.enc.3.block.mlp.fc1.bias', 'model.hid.enc.3.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.3.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.3.block.mlp.fc2.weight', 'model.hid.enc.3.block.mlp.fc2.bias', 'model.hid.enc.4.block.layer_scale_1', 'model.hid.enc.4.block.layer_scale_2', 'model.hid.enc.4.block.norm1.weight', 'model.hid.enc.4.block.norm1.bias', 'model.hid.enc.4.block.norm1.running_mean', 'model.hid.enc.4.block.norm1.running_var', 'model.hid.enc.4.block.norm1.num_batches_tracked', 'model.hid.enc.4.block.attn.proj_1.weight', 'model.hid.enc.4.block.attn.proj_1.bias', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.4.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.4.block.attn.proj_2.weight', 'model.hid.enc.4.block.attn.proj_2.bias', 'model.hid.enc.4.block.norm2.weight', 'model.hid.enc.4.block.norm2.bias', 'model.hid.enc.4.block.norm2.running_mean', 'model.hid.enc.4.block.norm2.running_var', 'model.hid.enc.4.block.norm2.num_batches_tracked', 'model.hid.enc.4.block.mlp.fc1.weight', 'model.hid.enc.4.block.mlp.fc1.bias', 'model.hid.enc.4.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.4.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.4.block.mlp.fc2.weight', 'model.hid.enc.4.block.mlp.fc2.bias', 'model.hid.enc.5.block.layer_scale_1', 'model.hid.enc.5.block.layer_scale_2', 'model.hid.enc.5.block.norm1.weight', 'model.hid.enc.5.block.norm1.bias', 'model.hid.enc.5.block.norm1.running_mean', 'model.hid.enc.5.block.norm1.running_var', 'model.hid.enc.5.block.norm1.num_batches_tracked', 'model.hid.enc.5.block.attn.proj_1.weight', 'model.hid.enc.5.block.attn.proj_1.bias', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.5.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.5.block.attn.proj_2.weight', 'model.hid.enc.5.block.attn.proj_2.bias', 'model.hid.enc.5.block.norm2.weight', 'model.hid.enc.5.block.norm2.bias', 'model.hid.enc.5.block.norm2.running_mean', 'model.hid.enc.5.block.norm2.running_var', 'model.hid.enc.5.block.norm2.num_batches_tracked', 'model.hid.enc.5.block.mlp.fc1.weight', 'model.hid.enc.5.block.mlp.fc1.bias', 'model.hid.enc.5.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.5.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.5.block.mlp.fc2.weight', 'model.hid.enc.5.block.mlp.fc2.bias', 'model.hid.enc.6.block.layer_scale_1', 'model.hid.enc.6.block.layer_scale_2', 'model.hid.enc.6.block.norm1.weight', 'model.hid.enc.6.block.norm1.bias', 'model.hid.enc.6.block.norm1.running_mean', 'model.hid.enc.6.block.norm1.running_var', 'model.hid.enc.6.block.norm1.num_batches_tracked', 'model.hid.enc.6.block.attn.proj_1.weight', 'model.hid.enc.6.block.attn.proj_1.bias', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.6.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.6.block.attn.proj_2.weight', 'model.hid.enc.6.block.attn.proj_2.bias', 'model.hid.enc.6.block.norm2.weight', 'model.hid.enc.6.block.norm2.bias', 'model.hid.enc.6.block.norm2.running_mean', 'model.hid.enc.6.block.norm2.running_var', 'model.hid.enc.6.block.norm2.num_batches_tracked', 'model.hid.enc.6.block.mlp.fc1.weight', 'model.hid.enc.6.block.mlp.fc1.bias', 'model.hid.enc.6.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.6.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.6.block.mlp.fc2.weight', 'model.hid.enc.6.block.mlp.fc2.bias', 'model.hid.enc.7.block.layer_scale_1', 'model.hid.enc.7.block.layer_scale_2', 'model.hid.enc.7.block.norm1.weight', 'model.hid.enc.7.block.norm1.bias', 'model.hid.enc.7.block.norm1.running_mean', 'model.hid.enc.7.block.norm1.running_var', 'model.hid.enc.7.block.norm1.num_batches_tracked', 'model.hid.enc.7.block.attn.proj_1.weight', 'model.hid.enc.7.block.attn.proj_1.bias', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv0.weight', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv0.bias', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv_spatial.weight', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv_spatial.bias', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv1.weight', 'model.hid.enc.7.block.attn.spatial_gating_unit.conv1.bias', 'model.hid.enc.7.block.attn.proj_2.weight', 'model.hid.enc.7.block.attn.proj_2.bias', 'model.hid.enc.7.block.norm2.weight', 'model.hid.enc.7.block.norm2.bias', 'model.hid.enc.7.block.norm2.running_mean', 'model.hid.enc.7.block.norm2.running_var', 'model.hid.enc.7.block.norm2.num_batches_tracked', 'model.hid.enc.7.block.mlp.fc1.weight', 'model.hid.enc.7.block.mlp.fc1.bias', 'model.hid.enc.7.block.mlp.dwconv.dwconv.weight', 'model.hid.enc.7.block.mlp.dwconv.dwconv.bias', 'model.hid.enc.7.block.mlp.fc2.weight', 'model.hid.enc.7.block.mlp.fc2.bias', 'model.hid.enc.7.reduction.weight', 'model.hid.enc.7.reduction.bias'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['state_dict'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "custom_training_config = {\n",
    "    'pre_seq_length': 15,\n",
    "    'aft_seq_length': 15,\n",
    "    'total_length': 15 + 15,\n",
    "    'batch_size': batch_size,\n",
    "    'val_batch_size': batch_size,\n",
    "    'epoch': 20,\n",
    "    'lr': 0.001,\n",
    "    'metrics': ['mse', 'mae'],\n",
    "\n",
    "    'ex_name': 'custom_exp',\n",
    "    'dataname': 'custom',\n",
    "    'in_shape': [15,3,64,64],\n",
    "}\n",
    "\n",
    "\n",
    "custom_model_config = {\n",
    "    # For MetaVP models, the most important hyperparameters are:\n",
    "    # N_S, N_T, hid_S, hid_T, model_type\n",
    "    'method': 'SimVP',\n",
    "    # Users can either using a config file or directly set these hyperparameters\n",
    "    # 'config_file': 'configs/custom/example_model.py',\n",
    "\n",
    "    # Here, we directly set these parameters\n",
    "    'model_type': 'gSTA',\n",
    "    'N_S': 4,\n",
    "    'N_T': 8,\n",
    "    'hid_S': 64,\n",
    "    'hid_T': 256\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openstl.utils import create_parser, default_parser\n",
    "\n",
    "args = create_parser().parse_args([])\n",
    "config = args.__dict__\n",
    "\n",
    "# update default parameters\n",
    "default_values = default_parser()\n",
    "for attribute in default_values.keys():\n",
    "    if config[attribute] is None:\n",
    "        config[attribute] = default_values[attribute]\n",
    "\n",
    "# update the training config\n",
    "config.update(custom_training_config)\n",
    "# update the model config\n",
    "config.update(custom_model_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openstl.methods import SimVP\n",
    "\n",
    "PATH = r'C:\\Users\\Omar\\Documents\\ShadowMitigation\\playground\\content\\checkpoints\\best.ckpt'\n",
    "\n",
    "model = SimVP(steps_per_epoch=1, \n",
    "            test_mean=1, \n",
    "            test_std=1, \n",
    "            save_dir=\"\", \n",
    "            **config)\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(PATH)  # Ensure it's on the right device\n",
    "\n",
    "# Load the adjusted state_dict into the model\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch, target = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_test(batch):\n",
    "\n",
    "    # Last 15 frames are target\n",
    "    target = np.array(batch)[:,45:]                     \n",
    "    \n",
    "    # Add channel dim, scale pixels between 0 and 1, send to GPU\n",
    "    batch = torch.tensor(batch).unsqueeze(1)          \n",
    "    batch = batch / 255.0                             \n",
    "    batch = batch.to(\"cuda\")                          \n",
    "    return batch, target\n",
    "\n",
    "# Test Data Loader\n",
    "\n",
    "test_loader = DataLoader(test_data,shuffle=True, \n",
    "                         batch_size=10, collate_fn=collate_test)\n",
    "\n",
    "# Get a batch\n",
    "batch, target = next(iter(test_loader))\n",
    "\n",
    "# Initialize output sequence\n",
    "output = np.zeros(target.shape, dtype=np.uint8)\n",
    "\n",
    "## Loop over timesteps\n",
    "#for timestep in tqdm(range(target.shape[1])):\n",
    "#  input = batch[:,:,timestep:timestep+15]   \n",
    "#  output[:,timestep]=(model(input).squeeze(1).cpu()>0.5)*255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 60, 64, 64] to have 3 channels, but got 60 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\openstl-1.0.0-py3.9.egg\\openstl\\methods\\simvp.py:23\u001b[0m, in \u001b[0;36mSimVP.forward\u001b[1;34m(self, batch_x, batch_y, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m pre_seq_length, aft_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39mpre_seq_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhparams\u001b[38;5;241m.\u001b[39maft_seq_length\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m aft_seq_length \u001b[38;5;241m==\u001b[39m pre_seq_length:\n\u001b[1;32m---> 23\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m aft_seq_length \u001b[38;5;241m<\u001b[39m pre_seq_length:\n\u001b[0;32m     25\u001b[0m     pred_y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(batch_x)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\openstl-1.0.0-py3.9.egg\\openstl\\models\\simvp_model.py:39\u001b[0m, in \u001b[0;36mSimVP_Model.forward\u001b[1;34m(self, x_raw, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m B, T, C, H, W \u001b[38;5;241m=\u001b[39m x_raw\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     37\u001b[0m x \u001b[38;5;241m=\u001b[39m x_raw\u001b[38;5;241m.\u001b[39mview(B\u001b[38;5;241m*\u001b[39mT, C, H, W)\n\u001b[1;32m---> 39\u001b[0m embed, skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m _, C_, H_, W_ \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m     42\u001b[0m z \u001b[38;5;241m=\u001b[39m embed\u001b[38;5;241m.\u001b[39mview(B, T, C_, H_, W_)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\openstl-1.0.0-py3.9.egg\\openstl\\models\\simvp_model.py:71\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):  \u001b[38;5;66;03m# B*4, 3, 128, 128\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m     enc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m     latent \u001b[38;5;241m=\u001b[39m enc1\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc)):\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\openstl-1.0.0-py3.9.egg\\openstl\\modules\\simvp_modules.py:77\u001b[0m, in \u001b[0;36mConvSC.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 77\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\openstl-1.0.0-py3.9.egg\\openstl\\modules\\simvp_modules.py:51\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 51\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_norm:\n\u001b[0;32m     53\u001b[0m         y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(y))\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Omar\\miniconda3\\envs\\shamit\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 3, 3], expected input[10, 60, 64, 64] to have 3 channels, but got 60 channels instead"
     ]
    }
   ],
   "source": [
    "model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffle(list(zip(target, output)))\n",
    "\n",
    "for tgt, out in zip(target, output):       # Loop over samples\n",
    "    \n",
    "    # Write target video as gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, tgt, \"GIF\", fps = 2)    \n",
    "        target_gif = gif.getvalue()\n",
    "\n",
    "    # Write output video as gif\n",
    "    with io.BytesIO() as gif:\n",
    "        imageio.mimsave(gif, out, \"GIF\", fps = 2)    \n",
    "        output_gif = gif.getvalue()\n",
    "\n",
    "    display(HBox([widgets.Image(value=target_gif), \n",
    "                  widgets.Image(value=output_gif)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shamit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
